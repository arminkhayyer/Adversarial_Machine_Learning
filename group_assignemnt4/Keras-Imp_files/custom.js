{"name": "Keras-Imp.ipynb", "path": "Keras-Imp.ipynb", "last_modified": "2019-11-17T23:08:52.346831Z", "created": "2019-11-17T23:08:52.346831Z", "content": {"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {"scrolled": false, "trusted": true}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Using TensorFlow backend.\n"}], "source": "# importing libraries \nfrom keras.preprocessing.image import ImageDataGenerator \nfrom keras.models import Sequential \nfrom keras.layers import Conv2D, MaxPooling2D, Reshape, UpSampling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras import backend as K \nimport pandas as pd \nimport numpy as np\nfrom PIL import Image\n\nimg_width, img_height = 224, 224\n\n# train_data_dir = r'C:\\Users\\mzn0041\\OneDrive - Auburn University\\PhD Auburn\\2019\\Data Science\\CNN\\Project\\CRACK500'\ntrain_data_dir= '/jet/prs/workspace/Data'\ndf_train = pd.read_csv(train_data_dir+'/train.txt', sep=' ', names= ['x_train', 'y_train'] )\ndf_test = pd.read_csv(train_data_dir+'/test.txt', sep=' ', names= ['x_test', 'y_test'] )\n"}, {"cell_type": "code", "execution_count": 2, "metadata": {"trusted": true}, "outputs": [], "source": "\nepochs = 10\nbatch_size = 32\ntrain_dir = '/jet/prs/workspace/Data'\n# train_dir = 'C:\\\\Users\\\\mzn0041\\\\OneDrive - Auburn University\\\\PhD Auburn\\\\2019\\\\Data Science\\\\CNN\\\\Project\\\\CRACK500'\n# test_dir = r'C:\\Users\\\\mzn0041\\\\OneDrive - Auburn University\\\\PhD Auburn\\\\2019\\\\Data Science\\\\CNN\\\\Project\\\\CRACK500'\n# if K.image_data_format() == 'channels_first': \n#     input_shape = (3, img_width, img_height) \n# else: \n#     input_shape = (img_width, img_height, 3)\n\ndef image_load(train_test, index):\n    if train_test == 0 :\n        ax = df_train['x_train'][index]\n        ax = '//' + ax\n        image = Image.open(train_dir + ax)\n    else:\n        ax = df_test['x_test'][index]\n        ax = '/' + ax\n        image = Image.open(train_dir+ ax) \n    image = image.resize((50,50 ))\n    image = np.array(image)/255\n    return image\n\n\ndef image_load_label(train_test, index):\n    if train_test == 0 :\n        ax = df_train['y_train'][index]\n        ax = '//' + ax\n        image = Image.open(train_dir+ ax)\n    else:\n        ax = df_test['y_test'][index]\n        ax = '//' + ax\n        image = Image.open(train_dir+ ax)\n    image = image.resize((50, 50))\n    image = np.array(image)\n    return image\n\nx_train = []\nfor i in range(len(df_train)):\n# for i in range(20):\n    x_train.append(image_load(0, i))\n\nx_test = []\nfor i in range(len(df_test)):\n# for i in range(20):\n    x_test.append(image_load(1, i))\n\n    \ny_test = []\nfor i in range(len(df_test)):\n# for i in range(20):\n    y_test.append(image_load_label(1, i))\n\n    \ny_train =[]\nfor i in range(len(df_train)):\n# for i in range(20):\n    y_train.append(image_load_label(0, i))\n\n    \n"}, {"cell_type": "code", "execution_count": 3, "metadata": {"trusted": true}, "outputs": [], "source": "x_train = np.array(x_train)\nx_test = np.array(x_test)\ny_train = np.array(y_train).reshape(len(df_train), -1)\ny_test = np.array(y_test).reshape(len(df_test), -1)"}, {"cell_type": "code", "execution_count": 34, "metadata": {"trusted": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "x_test    testcrop/20160222_080933_361_1.jpg\ny_test    testcrop/20160222_080933_361_1.png\nName: 0, dtype: object\n"}], "source": "img_ = x_test[0] *255\nprint (df_test.iloc[0])\nimg_ = Image.fromarray(img_, \"RGB\")\nimg_ = img_.resize((360, 640))\nimg_.save(\"test.png\")"}, {"cell_type": "code", "execution_count": 5, "metadata": {"trusted": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 48, 48, 32)        896       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 48, 48, 32)        0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 22, 22, 34)        9826      \n_________________________________________________________________\nactivation_2 (Activation)    (None, 22, 22, 34)        0         \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 7, 7, 34)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 1666)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 625)               1041875   \n_________________________________________________________________\nreshape_1 (Reshape)          (None, 25, 25, 1)         0         \n_________________________________________________________________\nup_sampling2d_1 (UpSampling2 (None, 50, 50, 1)         0         \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 2500)              0         \n_________________________________________________________________\nactivation_3 (Activation)    (None, 2500)              0         \n=================================================================\nTotal params: 1,052,597\nTrainable params: 1,052,597\nNon-trainable params: 0\n_________________________________________________________________\n"}], "source": "from keras.layers import Conv2DTranspose\nimport keras\nimport tensorflow as tf\n\ninput_shape = (360, 640, 3)\nmodel = Sequential() \nmodel.add(Conv2D(32, (3, 3), input_shape = x_train.shape[1:])) \nmodel.add(Activation('relu')) \nmodel.add(MaxPooling2D(pool_size = (2, 2))) \n\nmodel.add(Conv2D(34, (3, 3), strides = (1, 1))) \nmodel.add(Activation('relu')) \nmodel.add(MaxPooling2D(pool_size =(3, 3))) \nmodel.add(Flatten())\nmodel.add(Dense(625))\nmodel.add(Reshape(( 25, 25, -1)))\n# model.add(Conv2D(64, (3, 3))) \n# model.add(Activation('relu')) \n# model.add(MaxPooling2D(pool_size =(2, 2))) \n\n# model.add(Dropout(0.2)) \n# model.add(Conv2DTranspose(16, (3,3), strides=(2, 2), padding='valid', activation=\"relu\"))\n# model.add(Conv2DTranspose(8, (3,3), strides=(2, 2), padding='valid', activation=\"sigmoid\"))\n# model.add(Conv2DTranspose(1, (8,8), strides=(2, 2), padding='valid', activation=\"relu\"))\n# model.add(Conv2DTranspose(1, (2,2), strides=(2, 2), padding='valid', activation=\"relu\"))\n# model.add(Conv2DTranspose(1, (4,4), strides=(2, 2), padding='valid', activation=\"relu\"))\n\nmodel.add(UpSampling2D((2,2)))\n# model.add(Activation('relu')) \n# model.add(UpSampling2D((4,4)))\n# model.add(Dense(120, activation='relu'))\nmodel.add(Flatten())\nmodel.add(Activation('sigmoid')) \n\n# model.add(Flatten()) \n# model.add(Dense(64))\n\n# model.add(Dropout(0.2)) \n# model.add(Reshape(40, 30))\n# model.add(UpSampling2D(160, 120))\n# model.add(Activation('relu')) \n# model.add(UpSampling2D(640, 360))\n# model.add(Activation('sigmoid')) \n\nmodel.compile(loss = \"binary_crossentropy\", \n                     optimizer = \"adam\", \n                   metrics =['accuracy']) \nmodel.summary()"}, {"cell_type": "code", "execution_count": 6, "metadata": {"trusted": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Epoch 1/200\n1896/1896 [==============================] - 3s 1ms/step - loss: 0.3162 - acc: 0.9137\nEpoch 2/200\n1896/1896 [==============================] - 0s 106us/step - loss: 0.2428 - acc: 0.9363\nEpoch 3/200\n1896/1896 [==============================] - 0s 108us/step - loss: 0.2377 - acc: 0.9363\nEpoch 4/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.2351 - acc: 0.9363\nEpoch 5/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.2328 - acc: 0.9363\nEpoch 6/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.2296 - acc: 0.9363\nEpoch 7/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.2214 - acc: 0.9363\nEpoch 8/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.2048 - acc: 0.9365\nEpoch 9/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.1854 - acc: 0.9376\nEpoch 10/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.1749 - acc: 0.9384\nEpoch 11/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.1672 - acc: 0.9396\nEpoch 12/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.1622 - acc: 0.9405\nEpoch 13/200\n1896/1896 [==============================] - 0s 116us/step - loss: 0.1578 - acc: 0.9417\nEpoch 14/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1534 - acc: 0.9426\nEpoch 15/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1520 - acc: 0.9432\nEpoch 16/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.1483 - acc: 0.9439\nEpoch 17/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.1467 - acc: 0.9446\nEpoch 18/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1441 - acc: 0.9452\nEpoch 19/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1424 - acc: 0.9458\nEpoch 20/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.1408 - acc: 0.9463\nEpoch 21/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.1388 - acc: 0.9469\nEpoch 22/200\n1896/1896 [==============================] - 0s 115us/step - loss: 0.1366 - acc: 0.9476\nEpoch 23/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1357 - acc: 0.9480\nEpoch 24/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.1340 - acc: 0.9486\nEpoch 25/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1328 - acc: 0.9490\nEpoch 26/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.1315 - acc: 0.9496\nEpoch 27/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1297 - acc: 0.9501\nEpoch 28/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.1284 - acc: 0.9507\nEpoch 29/200\n1896/1896 [==============================] - 0s 115us/step - loss: 0.1271 - acc: 0.9511\nEpoch 30/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1267 - acc: 0.9514\nEpoch 31/200\n1896/1896 [==============================] - 0s 115us/step - loss: 0.1254 - acc: 0.9519\nEpoch 32/200\n1896/1896 [==============================] - 0s 116us/step - loss: 0.1232 - acc: 0.9526\nEpoch 33/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.1226 - acc: 0.9529\nEpoch 34/200\n1896/1896 [==============================] - 0s 115us/step - loss: 0.1217 - acc: 0.9533\nEpoch 35/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1204 - acc: 0.9538\nEpoch 36/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.1212 - acc: 0.9534\nEpoch 37/200\n1896/1896 [==============================] - 0s 115us/step - loss: 0.1209 - acc: 0.9537\nEpoch 38/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.1179 - acc: 0.9548\nEpoch 39/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1157 - acc: 0.9556\nEpoch 40/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.1147 - acc: 0.9561\nEpoch 41/200\n1896/1896 [==============================] - 0s 117us/step - loss: 0.1142 - acc: 0.9564\nEpoch 42/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1137 - acc: 0.9565\nEpoch 43/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1112 - acc: 0.9574\nEpoch 44/200\n1896/1896 [==============================] - 0s 115us/step - loss: 0.1110 - acc: 0.9575\nEpoch 45/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1099 - acc: 0.9581\nEpoch 46/200\n1896/1896 [==============================] - 0s 116us/step - loss: 0.1088 - acc: 0.9585\nEpoch 47/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1072 - acc: 0.9590\nEpoch 48/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1060 - acc: 0.9596\nEpoch 49/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.1054 - acc: 0.9598\nEpoch 50/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1038 - acc: 0.9604\nEpoch 51/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.1025 - acc: 0.9612\nEpoch 52/200\n1896/1896 [==============================] - 0s 115us/step - loss: 0.1012 - acc: 0.9616\nEpoch 53/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.1010 - acc: 0.9617\nEpoch 54/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0987 - acc: 0.9625\nEpoch 55/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0968 - acc: 0.9634\nEpoch 56/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0957 - acc: 0.9637\nEpoch 57/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.0944 - acc: 0.9644\nEpoch 58/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0925 - acc: 0.9652\nEpoch 59/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0909 - acc: 0.9657\nEpoch 60/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0895 - acc: 0.9664\nEpoch 61/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0876 - acc: 0.9671\nEpoch 62/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0864 - acc: 0.9676\nEpoch 63/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.0851 - acc: 0.9682\nEpoch 64/200\n1896/1896 [==============================] - 0s 115us/step - loss: 0.0833 - acc: 0.9688\nEpoch 65/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.0818 - acc: 0.9694\nEpoch 66/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0806 - acc: 0.9699\nEpoch 67/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0793 - acc: 0.9702\nEpoch 68/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0783 - acc: 0.9706\nEpoch 69/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0760 - acc: 0.9716\nEpoch 70/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0746 - acc: 0.9720\nEpoch 71/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0733 - acc: 0.9725\nEpoch 72/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.0725 - acc: 0.9728\nEpoch 73/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.0712 - acc: 0.9731\nEpoch 74/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0694 - acc: 0.9738\nEpoch 75/200\n1896/1896 [==============================] - 0s 110us/step - loss: 0.0684 - acc: 0.9741\nEpoch 76/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.0671 - acc: 0.9746\nEpoch 77/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.0662 - acc: 0.9748\nEpoch 78/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0655 - acc: 0.9750\nEpoch 79/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0642 - acc: 0.9754\nEpoch 80/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0627 - acc: 0.9759\nEpoch 81/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0619 - acc: 0.9761\nEpoch 82/200\n"}, {"name": "stdout", "output_type": "stream", "text": "1896/1896 [==============================] - 0s 110us/step - loss: 0.0607 - acc: 0.9764\nEpoch 83/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0596 - acc: 0.9767\nEpoch 84/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0587 - acc: 0.9770\nEpoch 85/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.0577 - acc: 0.9773\nEpoch 86/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0568 - acc: 0.9775\nEpoch 87/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0561 - acc: 0.9777\nEpoch 88/200\n1896/1896 [==============================] - 0s 115us/step - loss: 0.0551 - acc: 0.9779\nEpoch 89/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0550 - acc: 0.9779\nEpoch 90/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0543 - acc: 0.9780\nEpoch 91/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0530 - acc: 0.9784\nEpoch 92/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0524 - acc: 0.9785\nEpoch 93/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0519 - acc: 0.9786\nEpoch 94/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0510 - acc: 0.9788\nEpoch 95/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0505 - acc: 0.9789\nEpoch 96/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0500 - acc: 0.9789\nEpoch 97/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0493 - acc: 0.9791\nEpoch 98/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0482 - acc: 0.9794\nEpoch 99/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0476 - acc: 0.9795\nEpoch 100/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0473 - acc: 0.9795\nEpoch 101/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0468 - acc: 0.9796\nEpoch 102/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0465 - acc: 0.9797\nEpoch 103/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0459 - acc: 0.9798\nEpoch 104/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0454 - acc: 0.9798\nEpoch 105/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0450 - acc: 0.9799\nEpoch 106/200\n1896/1896 [==============================] - 0s 116us/step - loss: 0.0444 - acc: 0.9800\nEpoch 107/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0439 - acc: 0.9801\nEpoch 108/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0437 - acc: 0.9801\nEpoch 109/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0436 - acc: 0.9800\nEpoch 110/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.0430 - acc: 0.9801\nEpoch 111/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.0429 - acc: 0.9801\nEpoch 112/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0425 - acc: 0.9802\nEpoch 113/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.0423 - acc: 0.9802\nEpoch 114/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0419 - acc: 0.9803\nEpoch 115/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0415 - acc: 0.9803\nEpoch 116/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0414 - acc: 0.9803\nEpoch 117/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0412 - acc: 0.9803\nEpoch 118/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.0410 - acc: 0.9803\nEpoch 119/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0407 - acc: 0.9804\nEpoch 120/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0405 - acc: 0.9804\nEpoch 121/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0404 - acc: 0.9804\nEpoch 122/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0403 - acc: 0.9804\nEpoch 123/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0403 - acc: 0.9803\nEpoch 124/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0400 - acc: 0.9804\nEpoch 125/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.0396 - acc: 0.9804\nEpoch 126/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0396 - acc: 0.9804\nEpoch 127/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.0393 - acc: 0.9805\nEpoch 128/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.0393 - acc: 0.9804\nEpoch 129/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0392 - acc: 0.9804\nEpoch 130/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0390 - acc: 0.9804\nEpoch 131/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0389 - acc: 0.9804\nEpoch 132/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.0388 - acc: 0.9804\nEpoch 133/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0386 - acc: 0.9805\nEpoch 134/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0386 - acc: 0.9804\nEpoch 135/200\n1896/1896 [==============================] - 0s 110us/step - loss: 0.0385 - acc: 0.9804\nEpoch 136/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0384 - acc: 0.9804\nEpoch 137/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0384 - acc: 0.9804\nEpoch 138/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0384 - acc: 0.9804\nEpoch 139/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0383 - acc: 0.9804\nEpoch 140/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.0383 - acc: 0.9804\nEpoch 141/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0383 - acc: 0.9804\nEpoch 142/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.0383 - acc: 0.9804\nEpoch 143/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.0382 - acc: 0.9804\nEpoch 144/200\n1896/1896 [==============================] - 0s 115us/step - loss: 0.0380 - acc: 0.9804\nEpoch 145/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0379 - acc: 0.9804\nEpoch 146/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0379 - acc: 0.9804\nEpoch 147/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.0379 - acc: 0.9804\nEpoch 148/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0377 - acc: 0.9805\nEpoch 149/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0375 - acc: 0.9805\nEpoch 150/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0375 - acc: 0.9805\nEpoch 151/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.0374 - acc: 0.9805\nEpoch 152/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0375 - acc: 0.9804\nEpoch 153/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0374 - acc: 0.9804\nEpoch 154/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0375 - acc: 0.9804\nEpoch 155/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0372 - acc: 0.9805\nEpoch 156/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0373 - acc: 0.9804\nEpoch 157/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.0372 - acc: 0.9805\nEpoch 158/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0370 - acc: 0.9805\nEpoch 159/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0369 - acc: 0.9805\nEpoch 160/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0369 - acc: 0.9805\nEpoch 161/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0370 - acc: 0.9805\nEpoch 162/200\n"}, {"name": "stdout", "output_type": "stream", "text": "1896/1896 [==============================] - 0s 113us/step - loss: 0.0371 - acc: 0.9804\nEpoch 163/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.0369 - acc: 0.9805\nEpoch 164/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0369 - acc: 0.9805\nEpoch 165/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0367 - acc: 0.9805\nEpoch 166/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0369 - acc: 0.9805\nEpoch 167/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0366 - acc: 0.9805\nEpoch 168/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0368 - acc: 0.9805\nEpoch 169/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.0368 - acc: 0.9805\nEpoch 170/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0367 - acc: 0.9805\nEpoch 171/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.0369 - acc: 0.9804\nEpoch 172/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0368 - acc: 0.9804\nEpoch 173/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0369 - acc: 0.9804\nEpoch 174/200\n1896/1896 [==============================] - 0s 111us/step - loss: 0.0368 - acc: 0.9804\nEpoch 175/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0370 - acc: 0.9804\nEpoch 176/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0368 - acc: 0.9804\nEpoch 177/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0367 - acc: 0.9804\nEpoch 178/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0367 - acc: 0.9804\nEpoch 179/200\n1896/1896 [==============================] - 0s 112us/step - loss: 0.0365 - acc: 0.9805\nEpoch 180/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.0365 - acc: 0.9805\nEpoch 181/200\n1896/1896 [==============================] - 0s 114us/step - loss: 0.0364 - acc: 0.9805\nEpoch 182/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0363 - acc: 0.9805\nEpoch 183/200\n1896/1896 [==============================] - 0s 113us/step - loss: 0.0363 - acc: 0.9805\nEpoch 184/200\n1896/1896 [==============================] - 0s 117us/step - loss: 0.0363 - acc: 0.9805\nEpoch 185/200\n1896/1896 [==============================] - 0s 116us/step - loss: 0.0364 - acc: 0.9805\nEpoch 186/200\n1896/1896 [==============================] - 0s 116us/step - loss: 0.0363 - acc: 0.9805\nEpoch 187/200\n1896/1896 [==============================] - 0s 118us/step - loss: 0.0363 - acc: 0.9805\nEpoch 188/200\n1896/1896 [==============================] - 0s 118us/step - loss: 0.0363 - acc: 0.9805\nEpoch 189/200\n1896/1896 [==============================] - 0s 117us/step - loss: 0.0363 - acc: 0.9805\nEpoch 190/200\n1896/1896 [==============================] - 0s 118us/step - loss: 0.0362 - acc: 0.9805\nEpoch 191/200\n1896/1896 [==============================] - 0s 118us/step - loss: 0.0362 - acc: 0.9805\nEpoch 192/200\n1896/1896 [==============================] - 0s 116us/step - loss: 0.0364 - acc: 0.9805\nEpoch 193/200\n1896/1896 [==============================] - 0s 116us/step - loss: 0.0367 - acc: 0.9803\nEpoch 194/200\n1896/1896 [==============================] - 0s 115us/step - loss: 0.0365 - acc: 0.9804\nEpoch 195/200\n1896/1896 [==============================] - 0s 116us/step - loss: 0.0365 - acc: 0.9804\nEpoch 196/200\n1896/1896 [==============================] - 0s 116us/step - loss: 0.0362 - acc: 0.9805\nEpoch 197/200\n1896/1896 [==============================] - 0s 115us/step - loss: 0.0361 - acc: 0.9806\nEpoch 198/200\n1896/1896 [==============================] - 0s 115us/step - loss: 0.0361 - acc: 0.9805\nEpoch 199/200\n1896/1896 [==============================] - 0s 115us/step - loss: 0.0361 - acc: 0.9805\nEpoch 200/200\n1896/1896 [==============================] - 0s 116us/step - loss: 0.0362 - acc: 0.9805\n"}], "source": "model.fit(x_train, y_train, epochs = 200, batch_size=64)\nmodel.save_weights('model_saved.h5') "}, {"cell_type": "code", "execution_count": 28, "metadata": {"trusted": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "1124/1124 [==============================] - 0s 86us/step\nLoss = 0.338681434609\nTest Accuracy = 0.937101414407\n"}], "source": "import matplotlib.pyplot as plt\npreds = model.evaluate(x = x_test, y = y_test)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))\n\nimg_test = x_test[0]\nimg_test = Image.fromarray(img_test, \"RGB\")\nimg_test = img_test.resize((360, 360))\nimg_test.save(\"img_test.png\")\nimg = model.predict(x_test[0].reshape(1, 50, 50, 3))\nimg = np.where(img >= .5, 255, 0)\nimg = Image.fromarray(img, \"RGB\")\nimg.save(\"img.png\")"}, {"cell_type": "code", "execution_count": null, "metadata": {"trusted": true}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.3"}}, "nbformat": 4, "nbformat_minor": 2}, "format": "json", "mimetype": null, "writable": true, "type": "notebook"}